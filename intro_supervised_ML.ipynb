{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11c423b",
   "metadata": {},
   "source": [
    "## Introduction to Supervised Machine Learning\n",
    "**Instructor:** Lisa-Marie Rolli, Volkamer Lab, Saarland University (lisa-marie.rolli@uni-saarland.de)\n",
    "\n",
    "\n",
    "**Workshop date:** 13th January 2026\n",
    "\n",
    "**Further reading**: A gentle introduction into statistical learning with Python or R can be found in the textbook [\"An introduction to statistical learning\" by James et al.](https://www.statlearning.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734988a",
   "metadata": {},
   "source": [
    "Machine learning (ML) is a research area in computer science, where algorithms are developed to learn from data. Generally, ML can be subdivided into four major realms: (i) supervised learning, (ii) unsupervised learning, (iii) semi-supervised learning, and (iv) reinforcement learning. In this notebook, we focus on supervised ML. In supervised ML, an algorithm learns to predict a specific *response* (e.g., toxicity) for a sample (e.g., compound) based on *features* of the sample (e.g., physicochemical propteries). The learning is based on the assumption that there is a functional relationship between the features and the response, which we can approximate. There exist a plethora of different algorithms, which mainly differ in their assumptions about the true relationship and their learning algorithm, i.e., how they approximate the true function. In this tutorial we will start with linear regression, which assumes a linear relationship between the features and the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6663a",
   "metadata": {},
   "source": [
    "### Data\n",
    "As mentioned above, the dataset must consist of *features*, i.e., descriptors of the samples, and a *response*, i.e., a measurement that we want to predict. The goal is to learn a relationship between the features and the response for a given *training set* of samples so that we can later predict for *unseen* samples. In particular, we always separate a hold out *test set*, which is used to evaluate how well our model predicts for samples that it has not seen during training.\n",
    "\n",
    "The most important step in the ML pipeline is the data processing. In the following, we first download the [AqSolDB dataset](https://doi.org/10.1038/s41597-019-0151-1) from our GitHub, as this is our toy dataset for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ad804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install rdkit\n",
    "    !pip install datasail\n",
    "    !pip install shap\n",
    "    !wget https://raw.githubusercontent.com/volkamerlab/Programming4RADAR/refs/heads/first_notebook_draft/Example_Data/solubility_values.tsv\n",
    "    !wget https://raw.githubusercontent.com/volkamerlab/Programming4RADAR/refs/heads/first_notebook_draft/Example_Data/smiles.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datasail.sail import datasail\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from rdkit import Chem, RDLogger \n",
    "from rdkit.Chem import Draw, Descriptors, rdMolDescriptors\n",
    "from rdkit.Chem import PandasTools\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shap\n",
    "shap.initjs()\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7709f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    smiles_df = pd.read_csv('Example_Data/smiles.tsv', sep = '\\t', index_col = 0)\n",
    "else:\n",
    "    smiles_df = pd.read_csv('smiles.tsv', sep = '\\t', index_col = 0)\n",
    "smiles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8dc0d",
   "metadata": {},
   "source": [
    "Most ML models are not able to directly process text, e.g., SMILES strings. Therefore, we calculate numerical descriptors that are easier for the model to understand. One of the most simple representations are physicochemical properties that we can calculate using RDKit. RDKit is a library that can (among other things) read SMILES and store them as a molecule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_caffeine = \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\"\n",
    "caffeine = Chem.MolFromSmiles(smiles_caffeine)\n",
    "Draw.MolsToImage([caffeine])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463057af",
   "metadata": {},
   "source": [
    "After reading the SMILES, we can also caluclate different physiochemical properties of the molcule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MolWt: {Descriptors.MolWt(caffeine)}\")\n",
    "print(f\"logP: {Descriptors.MolLogP(caffeine)}\")\n",
    "print(f\"Num HBA: {rdMolDescriptors.CalcNumHBA(caffeine)}\")\n",
    "print(f\"Num HBD: {rdMolDescriptors.CalcNumHBD(caffeine)}\")\n",
    "print(f\"Num rotable bonds (RB): {rdMolDescriptors.CalcNumRotatableBonds(caffeine)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c367f27",
   "metadata": {},
   "source": [
    "Now, we want to calculate the LogP for all molecules in our ```smiles_df``` data frame, as we think, it should have some predictive information that can be leveraged when prediciting solubility. First, we convert all SMILES strings in our dataframe to molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that molecules are displayed in dataframe\n",
    "PandasTools.RenderImagesInAllColumns = True\n",
    "\n",
    "PandasTools.AddMoleculeColumnToFrame(smiles_df, \"SMILES\", molCol='molecule')\n",
    "smiles_df.dropna(inplace = True)\n",
    "smiles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79437f",
   "metadata": {},
   "source": [
    "Then, we calculate the LogP for all molecules in the dataframe. If we were not able to calculate the LogP for any of the comounds, we remove it, because we don't have a descriptor that we could use for our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_df[\"logp\"] = smiles_df[\"molecule\"].apply(Descriptors.MolLogP)\n",
    "# if we have missing values, we remove the molecules\n",
    "smiles_df.dropna(inplace=True)\n",
    "smiles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110d706",
   "metadata": {},
   "source": [
    "Now, we load the target, i.e., the continuous solubility values LogS, that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    response_df = pd.read_csv('Example_Data/solubility_values.tsv', sep = '\\t', index_col = 0)\n",
    "else:\n",
    "    response_df = pd.read_csv('solubility_values.tsv', sep = '\\t', index_col = 0)\n",
    "response_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b822f39",
   "metadata": {},
   "source": [
    "As not all molecules are necessarily tested in all assays, we now only take the intersection of molecules with available properties and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81940576",
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_mols = list(set(smiles_df.index).intersection(set(response_df.index)))\n",
    "X = smiles_df.loc[considered_mols, ['logp']]\n",
    "y = response_df.loc[considered_mols]\n",
    "print(f'We have {len(considered_mols)} molecules in our dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f087b",
   "metadata": {},
   "source": [
    "Now, we can have a look at the response plotted with respect to the different features (physicochemical properties) that we calculated before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=len(X.columns), figsize=(10, 5), sharex=False, sharey=True)\n",
    "\n",
    "\n",
    "ax.scatter(X, y)\n",
    "ax.set(xlabel='LogP')\n",
    "ax.set(ylabel=\"LogS\")\n",
    "# Optional: add a little padding so points on the edge arenâ€™t cut off\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e669d6d",
   "metadata": {},
   "source": [
    "## Train and Test Set\n",
    "As already mentioned above, we need two disjoint sets for training and for testing. Typically, the training set accounts for ~80% of the original dataset, while the remaining 20% are used for testing. The most easy way to split the data is to split the compounds randomly. However, as we want the model to generalize well across the chemical space, we make sure that similar compounds will be in the same set. To this end, we use a software called [`datasail`](https://datasail.readthedocs.io/en/latest/) that calculates the distance between the molecular fingerprints and splits the data so that the distance between the two sets is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_splits, _, _ = datasail(\n",
    "    techniques=[\"C1e\"],\n",
    "    splits=[8, 2],\n",
    "    names=[\"train\", \"test\"],\n",
    "    runs=3,\n",
    "    solver=\"SCIP\",\n",
    "    e_type=\"M\",\n",
    "    e_data=dict(zip(smiles_df.index, smiles_df[\"SMILES\"].values.tolist())),\n",
    ")\n",
    "split = e_splits['C1e'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54737e62",
   "metadata": {},
   "source": [
    "Now, we use the datasail output to split our features and responses into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for sample in split.keys():\n",
    "    if split[sample] == 'train':\n",
    "        train_samples.append(sample)\n",
    "    else:\n",
    "        test_samples.append(sample)\n",
    "\n",
    "X_train = X.loc[train_samples, :]\n",
    "X_test = X.loc[test_samples, :]\n",
    "y_train = y.loc[train_samples, :]\n",
    "y_test = y.loc[test_samples, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a911ff18",
   "metadata": {},
   "source": [
    "And we plot the train and test distribution for our feature (LogP) and response (LogS.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(20, 5), sharex='row', sharey=True)\n",
    "\n",
    "\n",
    "ax[0].scatter(X_train, y_train, label = 'Train set')\n",
    "ax[0].set(xlabel='LogP', ylabel = 'LogS')\n",
    "ax[1].scatter(X_test, y_test, label=\"Test set\")\n",
    "ax[1].set(xlabel='LogP')\n",
    "ax[0].set(title = 'Train set')\n",
    "ax[1].set(title = 'Test set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d7116",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We fit on the training set and predict on the test set. For the fit, we use *least square regression*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b851cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b468766b",
   "metadata": {},
   "source": [
    "The least square regression calculates the squared difference between all training datapoints and the current line (the prediction) and adjusts the slope and intercept of the line so that the difference between actual datapoints and prediction is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Scatter points\n",
    "ax.scatter(X_train, y_train, label=\"Training points\")\n",
    "\n",
    "train_prediction = regressor.predict(X_train)\n",
    "# Regression line\n",
    "ax.plot(X_train, train_prediction, color=\"tab:orange\", lw=2.5)\n",
    "\n",
    "# Residuals: vertical segments from each point to the line at the same x\n",
    "for xi, yi, yhi in zip(X_train.values, y_train.values, train_prediction):\n",
    "    ax.vlines(xi, ymin=min(yi, yhi), ymax=max(yi, yhi),\n",
    "                color=\"gray\", linestyle=\"--\", lw=1.5)\n",
    "\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(\"LogP\")\n",
    "ax.set_ylabel(\"LogS\")\n",
    "title = \"Least Squares Illustration (vertical residuals)\"\n",
    "\n",
    "ax.set_title(title)\n",
    "ax.margins(x=0.05, y=0.1)\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0fd1d",
   "metadata": {},
   "source": [
    "After fitting the regression line, we can use it to predict for any new data point in our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,  figsize=(20,5), sharex='row', sharey=True)\n",
    "\n",
    "\n",
    "ax[0].scatter(X_train, y_train, label = 'Train set')\n",
    "ax[0].set(xlabel='LogP', ylabel = 'LogS')\n",
    "ax[0].plot(X_train,regressor.predict(X_train),linewidth=3,color=\"tab:orange\", label=\"Model predictions\")\n",
    "ax[1].scatter(X_test, y_test, label=\"Test set\")\n",
    "ax[1].set(xlabel='LogP')\n",
    "ax[1].plot(X_test, y_pred, linewidth=3, color=\"tab:orange\", label=\"Model predictions\")\n",
    "\n",
    "ax[0].set(title = 'Train set')\n",
    "ax[1].set(title = 'Test set')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9b559",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "From looking at this plot, we can guess that this model does not predict really well. To quantify how good/bad a model *performs*, we use performance metrics such as the mean absolute error (MAE), which tells us how much our prediction $\\hat{y}_i$ for a sample $i$ differs from the true value $y_i$ averaged over the number of samples $n$:\n",
    "\n",
    "$MAE = \\frac{1}{n} \\cdot \\sum_{i = 1}^{n}\\vert \\hat{y}_i - y_i\\vert$\n",
    "\n",
    "The value is in the same unit as the response (in our case the LogS). Thus, the judgement on whether the observed value is good or bad depends on the dataset. Another metric that is easier to interpret is the *coefficient of determination ($R^2$)*. It is a value in $(-\\inf, 1]$, which tells you whether your model was better ($>0$) or worse ($<0$) than simply predicting the mean $\\bar{y}$ of the test set.\n",
    "\n",
    "$R^2 = \\frac{\\sum_{i = 1}^{n} (\\hat{y}_i - y_i)^2}{\\sum_{i = 1}^{n} (\\bar{y} - y_i)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"Coefficient of determination: {r2_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8badfe4",
   "metadata": {},
   "source": [
    "The metrics reveal what we could already guess from our plot: The model does not perform well. Remember, however, that we only predict the LogS based on the LogP. Thus, the next step is to include all [217 physicochemical descriptors](https://www.rdkit.org/docs/source/rdkit.Chem.Descriptors.html#rdkit.Chem.Descriptors.CalcMolDescriptors) to obtain better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07603f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_df['descriptors'] = smiles_df['molecule'].apply(Descriptors.CalcMolDescriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196afc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = smiles_df['descriptors'].apply(pd.Series)\n",
    "X.dropna(inplace=True)\n",
    "X = X.astype('float32')\n",
    "X = X.loc[:, ~X.isin([-np.inf,np.inf ]).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0748ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.loc[X.index, :]\n",
    "\n",
    "train_samples = list(set(train_samples).intersection(set(X.index)))\n",
    "test_samples = list(set(test_samples).intersection(set(X.index)))\n",
    "\n",
    "X_train = X.loc[train_samples, :]\n",
    "X_test = X.loc[test_samples, :]\n",
    "y_train = y.loc[train_samples, :]\n",
    "y_test = y.loc[test_samples, :]\n",
    "\n",
    "# when using more than one feature, we standardize them so that \n",
    "# the model does not give to much weight to features with \n",
    "# naturally higher ranges \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linear_model = LinearRegression(n_jobs=-1, fit_intercept=True, positive=True)\n",
    "linear_model.fit(X_train_scaled, y_train)\n",
    "y_pred = linear_model.predict(X_test_scaled)\n",
    "print(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"Coefficient of determination: {r2_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69798b",
   "metadata": {},
   "source": [
    "## More Advanced Models\n",
    "As we can see, the model trained on all descriptors actually performs worse than the one only trained on LogP. Thus, we conclude that our linear model could not capture the real underlying complexity of our data, as we assume linearity. Therefore, we now use a more complex model, i.e., a random forest (RF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f474287",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42, n_estimators=500, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"Coefficient of determination: {r2_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033216dc",
   "metadata": {},
   "source": [
    "As we can see, the RF works well for this dataset with a $R^2$ of around 0.7. Maybe using a even more complex model, e.g., a neural network improves the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(random_state = 42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "print(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"Coefficient of determination: {r2_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12d8e0",
   "metadata": {},
   "source": [
    "As we can see, the neural network did not perform as good as the RF, which might be because of the limited number of samples (<10,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8fbdab",
   "metadata": {},
   "source": [
    "## Interpretability of ML Models\n",
    "\n",
    "The RF was the best model in our analysis. Yet, we do not know how it comes to its predictions, but we just know what it outputs. To better understand its decisions, we now analyse the importance of the different features (i.e., physchem properties) for our prediciton. To this end, we use [shapley values](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html) that measure the feature importance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8009164",
   "metadata": {},
   "source": [
    "## Even More Advanced Workflows\n",
    "\n",
    "When working with ML models, we always have parameters that we have to adjust based on the given data, e.g., for the linear model we adjusted the slope and the y-intercept. When having more complex models there are also so-called hyperparameters, which are the parameters that already define parts of the model, e.g., the number of trees in a random forest. The hyperparameters have to be selected before training, but ideally we would like to select them based on the data. Therefore, we use a method called k-fold CV, where we split the training set into k parts and in each of the k iterations, we use another part for testing and the remaining k-1 parts for training. By this we can already estimate the error on the real test set and make informed decisions without leaking information from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21f175",
   "metadata": {},
   "source": [
    "First, we split the training data into 5 folds for a 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08500f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_splits, _, _ = datasail(\n",
    "    techniques=[\"C1e\"],\n",
    "    splits=[0.2 for i in range(5)],\n",
    "    names=[f'fold{i}' for i in range(1,6)],\n",
    "    runs=3,\n",
    "    solver=\"SCIP\",\n",
    "    e_type=\"M\",\n",
    "    e_data=dict(zip(X_train.index, smiles_df.loc[X_train.index, \"SMILES\"].values.tolist())),\n",
    ")\n",
    "split = e_splits['C1e'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb6086",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = {}\n",
    "for sample in split.keys():\n",
    "    fold = split[sample]\n",
    "    if not fold in folds.keys():\n",
    "        folds[fold] = [sample]\n",
    "    else:\n",
    "        folds[fold].append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -np.inf\n",
    "best_trees = None\n",
    "for n_trees in [200, 500, 700]:\n",
    "    r2s = []\n",
    "    for i in range(1,6):\n",
    "        fold = f'fold{i}'\n",
    "        test_part = X_train.loc[folds[fold], :]\n",
    "        train_folds = set(f'fold{j}' for j in range(1,6))\n",
    "        train_folds.remove(fold)\n",
    "        train_part_samples = np.concatenate([folds[f] for f in train_folds])\n",
    "        train_part = X_train.loc[train_part_samples, :]\n",
    "        train_response = y_train.loc[train_part_samples, :]\n",
    "        test_response = y_train.loc[folds[fold], :]\n",
    "        rf = RandomForestRegressor(random_state=42, n_estimators=n_trees, n_jobs=-1)\n",
    "        rf.fit(train_part, train_response)\n",
    "        test_predict = rf.predict(test_part)\n",
    "        r2s.append(r2_score(test_predict, test_response))\n",
    "    mean_performance = np.mean(r2s)\n",
    "    print(f'5-fold CV done for #trees = {n_trees}.')\n",
    "    print(f'Average performance of {mean_performance}.')\n",
    "    if  mean_performance > best_score:\n",
    "        best_score = mean_performance\n",
    "        best_trees = n_trees\n",
    "print(f'The best number of trees is {best_trees}.')\n",
    "print(f'The model had on average an R2 of {best_score}.')\n",
    "rf_final = RandomForestRegressor(random_state=42, n_estimators=best_trees)\n",
    "rf_final.fit(X_train,y_train)\n",
    "y_pred = rf_final.predict(X_test)\n",
    "print('Performance on the test set:')\n",
    "print(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"Coefficient of determination: {r2_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5101b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
